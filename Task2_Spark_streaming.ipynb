{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.1\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, decode, expr\n",
    "\n",
    "conf = SparkConf().setAppName(\"Assignment-2B-Task2_spark_streaming \").setMaster(\"local[2]\").set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.2\n",
    "\n",
    "#injesting bureau data\n",
    "bureau_stream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\", 'Bureau_data') \\\n",
    "  .option(\"startingOffsets\", \"latest\")\\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#injesting customer data\n",
    "topic = 'Customer_Data'\n",
    "customer_stream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\",topic) \\\n",
    "  .option(\"startingOffsets\", \"latest\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume all data is string\n",
    "df_bureau = bureau_stream.selectExpr( \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = customer_stream.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.3\n",
    "#Defining the Schema for Customer dataframe\n",
    "from pyspark.sql.types import StructField,IntegerType, StructType,StringType, DoubleType\n",
    "schema = ArrayType(StructType([\n",
    "       StructField('ID',StringType(),True),\n",
    "       StructField('Frequency',StringType(),True),\n",
    "       StructField('InstlmentMode',StringType(),True),\n",
    "       StructField('LoanStatus',StringType(),True),\n",
    "       StructField('PaymentMode',StringType(),True),\n",
    "       StructField('BranchID',StringType(),True),\n",
    "       StructField('Area',StringType(),True) ,\n",
    "       StructField('Tenure',StringType(),True), \n",
    "       StructField('AssetCost',StringType(),True) ,\n",
    "       StructField('AmountFinance',StringType(),True) ,\n",
    "       StructField('DisbursalAmount',StringType(),True), \n",
    "       StructField('EMI',StringType(),True), \n",
    "       StructField('DisbursalDate',StringType(),True) ,\n",
    "       StructField('MaturityDAte',StringType(),True), \n",
    "       StructField('AuthDate',StringType(),True), \n",
    "       StructField('AssetID',StringType(),True),\n",
    "       StructField('ManufacturerID',StringType(),True),\n",
    "       StructField('SupplierID',StringType(),True),\n",
    "       StructField('LTV',StringType(),True), \n",
    "       StructField('SEX',StringType(),True), \n",
    "       StructField('AGE',StringType(),True), \n",
    "       StructField('MonthlyIncome',StringType(),True), \n",
    "       StructField('City',StringType(),True),\n",
    "       StructField('State',StringType(),True),\n",
    "       StructField('ZiPCODE',StringType(),True),\n",
    "       StructField('Top-up Month',StringType(),True),\n",
    "       StructField('ts',StringType(),True)\n",
    "       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the customer data\n",
    "df_customer = df_customer.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))\n",
    "df_customer = df_customer.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = df_customer.select(\n",
    "                    F.col(\"unnested_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"unnested_value.Frequency\").alias(\"Frequency\"),\n",
    "                    F.col(\"unnested_value.InstlmentMode\").alias(\"InstlmentMode\"),\n",
    "                    F.col(\"unnested_value.LoanStatus\").alias(\"LoanStatus\"),\n",
    "                    F.col(\"unnested_value.PaymentMode\").alias(\"PaymentMode\"),\n",
    "                    F.col(\"unnested_value.BranchID\").alias(\"BranchID\"),\n",
    "                    F.col(\"unnested_value.Area\").alias(\"Area\"),\n",
    "                    F.col(\"unnested_value.Tenure\").alias(\"Tenure\"),\n",
    "                    F.col(\"unnested_value.InstlmentMode\").alias(\"InstlmentMode\"),\n",
    "                    F.col(\"unnested_value.AssetCost\").alias(\"AssetCost\"),\n",
    "                    F.col(\"unnested_value.AmountFinance\").alias(\"AmountFinance\"),\n",
    "                    F.col(\"unnested_value.DisbursalAmount\").alias(\"DisbursalAmount\"),\n",
    "                    F.col(\"unnested_value.EMI\").alias(\"EMI\"),\n",
    "                    F.col(\"unnested_value.DisbursalDate\").alias(\"DisbursalDate\"),\n",
    "                    F.col(\"unnested_value.MaturityDAte\").alias(\"MaturityDAte\"),\n",
    "                    F.col(\"unnested_value.AuthDate\").alias(\"AuthDatee\"),\n",
    "                    F.col(\"unnested_value.AssetID\").alias(\"AssetID\"),\n",
    "                    F.col(\"unnested_value.ManufacturerID\").alias(\"ManufacturerID\"),\n",
    "                    F.col(\"unnested_value.SupplierID\").alias(\"SupplierID\"),\n",
    "                    F.col(\"unnested_value.LTV\").alias(\"LTV\"),\n",
    "                    F.col(\"unnested_value.SEX\").alias(\"SEX\"),\n",
    "                    F.col(\"unnested_value.AGE\").alias(\"AGE\"),\n",
    "                    F.col(\"unnested_value.MonthlyIncome\").alias(\"MonthlyIncome\"),\n",
    "                    F.col(\"unnested_value.City\").alias(\"City\"),\n",
    "                    F.col(\"unnested_value.State\").alias(\"State\"),\n",
    "                    F.col(\"unnested_value.ZiPCODE\").alias(\"ZiPCODE\"),\n",
    "                    F.col(\"unnested_value.Top-up Month\").alias(\"Top-up Month\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")                    \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casting the non-string columns to appropriate datatypes\n",
    "df_customer = df_customer.withColumn(\"ID\", col(\"ID\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "df_customer = df_customer.withColumn(\"Tenure\", col(\"Tenure\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "df_customer = df_customer.withColumn(\"AssetCost\", col(\"AssetCost\").cast(IntegerType()))\n",
    "df_customer = df_customer.withColumn(\"AmountFinance\", col(\"AmountFinance\").cast(DoubleType()))\n",
    "df_customer = df_customer.withColumn(\"DisbursalAmount\", col(\"DisbursalAmount\").cast(DoubleType()))\n",
    "df_customer = df_customer.withColumn(\"EMI\", col(\"EMI\").cast(DoubleType()))\n",
    "\n",
    "df_customer = df_customer.withColumn(\"LTV\", col(\"LTV\").cast(DoubleType()))\n",
    "df_customer = df_customer.withColumn(\"AGE\", col(\"AGE\").cast(IntegerType()))\n",
    "df_customer = df_customer.withColumn(\"MonthlyIncome\", col(\"MonthlyIncome\").cast(DoubleType()))\n",
    "\n",
    "df_customer = df_customer.withColumn(\"ZiPCODE\", col(\"ZiPCODE\").cast(IntegerType()))\n",
    "df_customer = df_customer.withColumn(\"ts\", col('ts').cast(IntegerType()))\n",
    "df_customer = df_customer.withColumn(\"ts\", from_unixtime('ts').alias('ts'))\n",
    "df_customer = df_customer.withColumn(\"ts\",col(\"ts\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Schema for Customer dataframe\n",
    "from pyspark.sql.types import StructField,IntegerType, StructType,StringType, DoubleType,BooleanType,TimestampType\n",
    "schema_b = ArrayType(StructType([\n",
    "       StructField('ID',StringType(),True),\n",
    "       StructField('SELF-INDICATOR',StringType(),True),\n",
    "       StructField('MATCH-TYPE',StringType(),True),\n",
    "       StructField('ACCT-TYPE',StringType(),True),\n",
    "       StructField('CONTRIBUTOR-TYPE',StringType(),True),\n",
    "       StructField('DATE-REPORTED',StringType(),True),\n",
    "       StructField('OWNERSHIP-IND',StringType(),True) ,\n",
    "       StructField('ACCOUNT-STATUS',StringType(),True), \n",
    "       StructField('DISBURSED-DT',StringType(),True) ,\n",
    "       StructField('CLOSE-DT',StringType(),True) ,\n",
    "       StructField('LAST-PAYMENT-DATE',StringType(),True), \n",
    "       StructField('CREDIT-LIMIT/SANC AMT',StringType(),True), \n",
    "       StructField('DISBURSED-AMT/HIGH CREDIT',StringType(),True) ,\n",
    "       StructField('INSTALLMENT-AMT',StringType(),True), \n",
    "       StructField('CURRENT-BAL',StringType(),True), \n",
    "       StructField('OVERDUE-AMT',StringType(),True),\n",
    "       StructField('WRITE-OFF-AMT',StringType(),True),\n",
    "       StructField('ASSET_CLASS',StringType(),True),\n",
    "       StructField('REPORTED DATE - HIST',StringType(),True), \n",
    "       StructField('DPD - HIST',StringType(),True), \n",
    "       StructField('CUR BAL - HIST',StringType(),True), \n",
    "       StructField('AMT OVERDUE - HIST',StringType(),True), \n",
    "       StructField('AMT PAID - HIST',StringType(),True),\n",
    "       StructField('TENURE',StringType(),True),\n",
    "       StructField('ts',StringType(),True)\n",
    "       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the bureau data \n",
    "df_bureau = df_bureau.select(F.from_json(F.col('value').cast('string'), schema_b).alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau = df_bureau.select(F.explode(F.col('parsed_value')).alias('unnested_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau = df_bureau.select(\n",
    "                    F.col(\"unnested_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"unnested_value.SELF-INDICATOR\").alias(\"SELF-INDICATOR\"),\n",
    "                    F.col(\"unnested_value.MATCH-TYPE\").alias(\"MATCH-TYPE\"),\n",
    "                    F.col(\"unnested_value.ACCT-TYPE\").alias(\"ACCT-TYPE\"),\n",
    "                    F.col(\"unnested_value.CONTRIBUTOR-TYPE\").alias(\"CONTRIBUTOR-TYPE\"),\n",
    "                    F.col(\"unnested_value.DATE-REPORTED\").alias(\"DATE-REPORTED\"),\n",
    "                    F.col(\"unnested_value.OWNERSHIP-IND\").alias(\"OWNERSHIP-IND\"),\n",
    "                    F.col(\"unnested_value.ACCOUNT-STATUS\").alias(\"ACCOUNT-STATUS\"),\n",
    "                    F.col(\"unnested_value.DISBURSED-DT\").alias(\"DISBURSED-DT\"),\n",
    "                    F.col(\"unnested_value.CLOSE-DT\").alias(\"CLOSE-DT\"),\n",
    "                    F.col(\"unnested_value.LAST-PAYMENT-DATE\").alias(\"LAST-PAYMENT-DATE\"),\n",
    "                    F.col(\"unnested_value.CREDIT-LIMIT/SANC AMT\").alias(\"CREDIT-LIMIT/SANC AMT\"),\n",
    "                    F.col(\"unnested_value.DISBURSED-AMT/HIGH CREDIT\").alias(\"DISBURSED-AMT/HIGH CREDIT\"),\n",
    "                    F.col(\"unnested_value.INSTALLMENT-AMT\").alias(\"INSTALLMENT-AMT\"),\n",
    "                    F.col(\"unnested_value.CURRENT-BAL\").alias(\"CURRENT-BAL\"),\n",
    "                    F.col(\"unnested_value.OVERDUE-AMT\").alias(\"OVERDUE-AMT\"),\n",
    "                    F.col(\"unnested_value.WRITE-OFF-AMT\").alias(\"WRITE-OFF-AMT\"),\n",
    "                    F.col(\"unnested_value.ASSET_CLASS\").alias(\"ASSET_CLASS\"),\n",
    "                    F.col(\"unnested_value.REPORTED DATE - HIST\").alias(\"REPORTED DATE - HIST\"),\n",
    "                    F.col(\"unnested_value.DPD - HIST\").alias(\"DPD - HIST\"),\n",
    "                    F.col(\"unnested_value.CUR BAL - HIST\").alias(\"CUR BAL - HIST\"),\n",
    "                    F.col(\"unnested_value.AMT OVERDUE - HIST\").alias(\"AMT OVERDUE - HIST\"),\n",
    "                    F.col(\"unnested_value.AMT PAID - HIST\").alias(\"AMT PAID - HIST\"),\n",
    "                    F.col(\"unnested_value.TENURE\").alias(\"TENURE\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")                    \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the ',' with '' in numeric columns before typecasting\n",
    "from pyspark.sql.functions import udf\n",
    "UDF = udf(lambda s: s.replace(',', '') if s else 'None', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type casting the chosen columns from String Type to appropriate Type\n",
    "\n",
    "from pyspark.sql.functions import col,when\n",
    "df_bureau = df_bureau.withColumn(\"DISBURSED-AMT/HIGH CREDIT\",UDF(col(\"DISBURSED-AMT/HIGH CREDIT\")))\n",
    "df_bureau = df_bureau.withColumn(\"DISBURSED-AMT/HIGH CREDIT\", col(\"DISBURSED-AMT/HIGH CREDIT\").cast(IntegerType()))\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"CURRENT-BAL\",UDF(col(\"CURRENT-BAL\")))\n",
    "df_bureau = df_bureau.withColumn(\"CURRENT-BAL\", col(\"CURRENT-BAL\").cast(IntegerType()))\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"CREDIT-LIMIT/SANC AMT\",UDF(col(\"CREDIT-LIMIT/SANC AMT\")))\n",
    "df_bureau = df_bureau.withColumn(\"CREDIT-LIMIT/SANC AMT\", col(\"CREDIT-LIMIT/SANC AMT\").cast(IntegerType()))\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"OVERDUE-AMT\",UDF(col(\"OVERDUE-AMT\")))\n",
    "df_bureau = df_bureau.withColumn(\"OVERDUE-AMT\", col(\"OVERDUE-AMT\").cast(IntegerType()))\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"CUR BAL - HIST\",UDF(col(\"CUR BAL - HIST\")))\n",
    "df_bureau = df_bureau.withColumn(\"CUR BAL - HIST\", col(\"CUR BAL - HIST\").cast(IntegerType()))\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"WRITE-OFF-AMT\",UDF(col(\"WRITE-OFF-AMT\")))\n",
    "df_bureau = df_bureau.withColumn(\"WRITE-OFF-AMT\", col(\"WRITE-OFF-AMT\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"ID\", col(\"ID\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"SELF-INDICATOR\", col(\"SELF-INDICATOR\").cast(BooleanType()))\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"TENURE\",UDF(col(\"TENURE\")))\n",
    "df_bureau = df_bureau.withColumn(\"TENURE\", col(\"TENURE\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau = df_bureau.withColumn(\"AMT PAID - HIST\",UDF(col(\"AMT PAID - HIST\")))\n",
    "df_bureau = df_bureau.withColumn(\"AMT PAID - HIST\", col(\"AMT PAID - HIST\").cast(IntegerType()))\n",
    "\n",
    "df_bureau = df_bureau.withColumn(\"AMT OVERDUE - HIST\",UDF(col(\"AMT OVERDUE - HIST\")))\n",
    "df_bureau = df_bureau.withColumn(\"AMT OVERDUE - HIST\", col(\"AMT OVERDUE - HIST\").cast(IntegerType()))\n",
    "df_bureau = df_bureau.withColumn(\"ts\",col(\"ts\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau = df_bureau.withColumn(\"ts\", from_unixtime('ts').alias('ts'))\n",
    "df_bureau = df_bureau.withColumn(\"ts\",col(\"ts\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding water mark\n",
    "df_customer = df_customer \\\n",
    "    .withWatermark(\"ts\", \"5 seconds\")\n",
    "df_bureau = df_bureau \\\n",
    "    .withWatermark(\"ts\", \"5 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.4\n",
    "\n",
    "#Defining a UDF to convert Top-up Month to numeric\n",
    "def rep_self(string):\n",
    "    if string == 'false':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    #Creating the UDF function\n",
    "UDF = udf(lambda z: rep_self(z),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau = df_bureau.withColumn('SELF-INDICATOR', UDF('SELF-INDICATOR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data\n",
    "df_bureau1 = df_bureau \\\n",
    "     .groupBy(df_bureau.ID,F.window(df_bureau.ts, \"30 seconds\"))\\\n",
    "     .agg(F.sum('WRITE-OFF-AMT').alias('WRITE-OFF-AMT_sum'),\n",
    "       F.sum('CURRENT-BAL').alias('CURRENT-BAL_sum'),\n",
    "       F.sum('DISBURSED-AMT/HIGH CREDIT').alias('DISBURSED-AMT/HIGH CREDIT_sum'),\n",
    "       F.sum('SELF-INDICATOR').alias('SELF-INDICATOR_sum'),\n",
    "      approx_count_distinct('MATCH-TYPE').alias('MATCH-TYPE_dist'),\n",
    "      approx_count_distinct('ACCT-TYPE').alias('ACCT-TYPE_dist'),\n",
    "      approx_count_distinct('CONTRIBUTOR-TYPE').alias('CONTRIBUTOR-TYPE_dist'),\n",
    "      approx_count_distinct('OWNERSHIP-IND').alias('OWNERSHIP-IND_dist'),\n",
    "      approx_count_distinct('ACCOUNT-STATUS').alias('ACCOUNT-STATUS_dist'),\n",
    "      approx_count_distinct('DISBURSED-DT').alias('DISBURSED-DT_dist'),\n",
    "      approx_count_distinct('CLOSE-DT').alias('CLOSE-DT_dist'),\n",
    "      approx_count_distinct('LAST-PAYMENT-DATE').alias('LAST-PAYMENT-DATE_dist'),\n",
    "      sum('CREDIT-LIMIT/SANC AMT').alias('CREDIT-LIMIT/SANC AMT_sum'),\n",
    "\n",
    "      approx_count_distinct('INSTALLMENT-AMT').alias('INSTALLMENT-AMT_dist'),\n",
    "\n",
    "      F.sum('OVERDUE-AMT').alias('OVERDUE-AMT_sum'),\n",
    "\n",
    "      approx_count_distinct('ASSET_CLASS').alias('ASSET_CLASS_dist'),\n",
    "      approx_count_distinct('REPORTED DATE - HIST').alias('REPORTED DATE - HIST_dist'),\n",
    "      approx_count_distinct('DPD - HIST').alias('DPD - HIST_dist'),\n",
    "      F.sum('CUR BAL - HIST').alias('CUR BAL - HIST_sum'),\n",
    "      F.sum('AMT OVERDUE - HIST').alias('AMT OVERDUE - HIST_dist'),\n",
    "      F.sum('TENURE').alias('TENURE_sum'),\n",
    "      F.sum('AMT PAID - HIST').alias('AMT PAID - HIST_dist')\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.5\n",
    "df_bureau1 = df_bureau1.withColumn('window_start', col(\"window.start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau1 = df_bureau1.withColumn('window_end', col(\"window.end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting columns which are used by the provided top-up model\n",
    "df_bureau2 = df_bureau1.select('ID','WRITE-OFF-AMT_sum','CURRENT-BAL_sum','DISBURSED-AMT/HIGH CREDIT_sum','SELF-INDICATOR_sum','ACCT-TYPE_dist','window_end','window_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer1 = df_customer.select('ID',\"EMI\",\"MonthlyIncome\",'AssetCost','DisbursalAmount',\"LoanStatus\",\"BranchID\",\"Area\",\"Tenure\",\"ManufacturerID\",\"City\",\"State\",'ts','Top-up Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining the 2 streaming dataframes \n",
    "from pyspark.sql.functions import col\n",
    "df_inner_join = df_customer1.join(df_bureau2, on=['ID'], how='inner').where((df_customer1['ts']<=df_bureau2['window_end'])|(df_customer1['ts']>=df_bureau2['window_start']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.6\n",
    "df_inner_join = df_inner_join.withColumn('Top-up_Month',col('Top-up Month'))\n",
    "df_inner_join1 =  df_inner_join.select('ID', 'window_start', 'window_end', 'ts','Top-up_Month')\n",
    "df_inner_join = df_inner_join.drop('Top-up Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into parquet file\n",
    "query_file_sink = df_inner_join1.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/df_inner_join1\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/df_inner_join1/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_sink_df = spark.read.parquet(\"parquet/df_inner_join1\")\n",
    "query_file_sink_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.7\n",
    "\n",
    "#loading the pipeline model\n",
    "from pyspark.ml import PipelineModel\n",
    "pipelineModel = PipelineModel.load('topup_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the predictions\n",
    "predictions = pipelineModel.transform(df_inner_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting columns to store into parquet\n",
    "df_inner_join2 = predictions.select('ID', 'window_start', 'window_end', 'ts','Top-up_Month','prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner_join2 = df_inner_join2.withColumn('Top-up_Month',col('Top-up_Month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into parquet file\n",
    "query_file_sink1 = df_inner_join2.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/df_11\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/df_11/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_sink_df1 = spark.read.parquet(\"parquet/df_11\")\n",
    "query_file_sink_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for parquet\n",
    "schem = StructType([\n",
    "       StructField('S',StringType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting necessary columns\n",
    "df11 = predictions.select(\"window_end\",\"State\",'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Question 2.8\n",
    "\n",
    "import pandas\n",
    "import json\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    #For non-empty dataframe\n",
    "    if df.count()>0:\n",
    "        \n",
    "        #Perform aggregation\n",
    "        df1 = df \\\n",
    "        .select(\"window_end\",\"State\",'prediction')\\\n",
    "        .groupBy('State','window_end')\\\n",
    "        .agg(F.sum('prediction').alias(\"count\"))\\\n",
    "        .select('window_end',F.to_json(F.struct('State','count')))\n",
    "        \n",
    "        #renaming columns\n",
    "        df1 = df1.withColumn('key',col('window_end'))\n",
    "        df1 = df1.withColumn('value',col(\"to_json(struct(State, count))\"))\n",
    "        df1 = df1.drop('window_end',\"to_json(struct(State, count))\")\n",
    "        df2 = df1.toPandas()\n",
    "        \n",
    "        #Converting to key value pair json \n",
    "        df_new = df2.set_index('key')['value'].to_json()\n",
    "        print(\"epoch_id:\",epoch_id)\n",
    "        print(\"Count:\",df.count())\n",
    "        print(df_new)\n",
    "        return df_new\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing the foreachbatch function and saving output to parquet\n",
    "new_df = df11.writeStream.format(\"parquet\").outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .option(\"path\", \"parquet/query1\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/query1/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading from parquet\n",
    "query_read = spark.readStream.schema(schem).parquet(\"parquet/query1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fe27c64b1c0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 7\n",
      "Count: 121\n",
      "{\"1666074810000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":26.0}\",\"1666074780000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":6.0}\"}\n",
      "epoch_id: 8\n",
      "Count: 50\n",
      "{\"1666074840000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":15.0}\"}\n",
      "epoch_id: 9\n",
      "Count: 63\n",
      "{\"1666074870000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":19.0}\"}\n",
      "epoch_id: 10\n",
      "Count: 65\n",
      "{\"1666074900000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":24.0}\"}\n",
      "epoch_id: 11\n",
      "Count: 104\n",
      "{\"1666074960000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":20.0}\",\"1666074930000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":20.0}\"}\n",
      "epoch_id: 12\n",
      "Count: 38\n",
      "{\"1666074990000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":19.0}\"}\n",
      "epoch_id: 13\n",
      "Count: 121\n",
      "{\"1666075050000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":17.0}\",\"1666075020000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":27.0}\"}\n",
      "epoch_id: 14\n",
      "Count: 46\n",
      "{\"1666075080000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":12.0}\"}\n",
      "epoch_id: 15\n",
      "Count: 58\n",
      "{\"1666075110000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":22.0}\"}\n",
      "epoch_id: 16\n",
      "Count: 121\n",
      "{\"1666075140000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":32.0}\",\"1666075170000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":15.0}\"}\n",
      "epoch_id: 17\n",
      "Count: 70\n",
      "{\"1666075200000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":30.0}\"}\n",
      "epoch_id: 18\n",
      "Count: 65\n",
      "{\"1666075230000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":16.0}\"}\n",
      "epoch_id: 19\n",
      "Count: 69\n",
      "{\"1666075260000\":\"{\\\"State\\\":\\\"MADHYA PRADESH\\\",\\\"count\\\":16.0}\"}\n"
     ]
    }
   ],
   "source": [
    "#Writing to kafka\n",
    "query_read\\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"topic\", \"stream_data\")\\\n",
    "    .option(\"checkpointLocation\", \"ckpt/demo\")\\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
